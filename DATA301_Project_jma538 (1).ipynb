{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA301 Data Project**\n",
        "\n",
        "The data project revolves around the reddit submission [dataset](https://snap.stanford.edu/data/web-Reddit.html). The purpose of this project is to find...."
      ],
      "metadata": {
        "id": "54kndmO41nQc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUiox8GRf51q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb5cf603-6397-4d88-f1fe-4bb925d341ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'against', 'been', \"weren't\", 'few', \"didn't\", 't', \"won't\", 'more', 'so', 'own', 'yourselves', \"you'll\", \"she'd\", 'i', 'mightn', 'not', 'too', \"needn't\", \"they're\", 'them', 'after', \"i'd\", 'same', 'about', 'd', \"they'd\", 'itself', 'other', \"we'd\", 'there', 'through', 'had', \"couldn't\", \"don't\", \"i'll\", \"she'll\", 'nor', 're', 'doing', 'can', 'again', 'down', 'from', 'isn', \"mustn't\", \"you've\", 'him', 'or', 'where', 'were', 'each', 'o', 'by', 'aren', 'out', 'your', \"shouldn't\", \"i've\", \"i'm\", 'should', 'ourselves', 'why', 'what', 'me', 'didn', 'that', 'theirs', 'themselves', 'yourself', 'myself', \"aren't\", 'to', 'between', 'y', \"it's\", 'be', 'which', 'those', 'while', \"hadn't\", \"we're\", 'couldn', 'just', 'some', 'under', 'ain', 'has', \"they've\", 'on', 'whom', \"that'll\", 'with', 'you', \"he'll\", 'her', 'any', 'they', 'who', 'how', 'herself', 'all', \"doesn't\", 'further', 'and', 'an', 'our', 'his', 'do', 'is', 'having', 'of', 'over', 'shouldn', \"wasn't\", 'was', \"hasn't\", 'wouldn', 'their', 'for', \"it'll\", 'needn', \"he'd\", 'only', 'very', \"we'll\", 'll', 'mustn', 'this', 'into', 'below', 'ma', \"you'd\", 'are', 'off', 'but', \"you're\", \"isn't\", 'as', 'haven', 'both', 'have', 'm', 'hadn', 'no', 's', 'yours', 'a', 'once', 'ours', \"should've\", 'these', 'until', 've', 'then', 'at', \"we've\", 'in', 'above', \"they'll\", 'it', 'doesn', 'will', \"she's\", 'when', 'won', 'being', 'here', 'now', 'he', \"wouldn't\", 'my', \"haven't\", 'she', 'because', 'am', \"shan't\", 'does', 'than', \"mightn't\", 'did', 'don', 'hers', 'himself', 'hasn', 'most', 'weren', 'wasn', 'im', 'we', 'during', \"he's\", 'shan', 'its', 'the', 'before', 'such', 'if', \"it'd\", 'up'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#import necessary items\n",
        "import dask\n",
        "from dask import bag as db\n",
        "from dask import dataframe as df\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#downloading stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "custom_stopwords = {'just', 'im', 'the'}\n",
        "stop_words1 = stop_words.union(custom_stopwords)\n",
        "print(stop_words1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setting up environment**\n",
        "Downloading the dataset"
      ],
      "metadata": {
        "id": "WgTwK0Bz2ana"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gzip\n",
        "import os\n",
        "\n",
        "if os.path.exists('redditSubmissions.csv.gz'):\n",
        "  print('File already exists')\n",
        "else:\n",
        "  !wget https://snap.stanford.edu/data/redditSubmissions.csv.gz\n",
        "  with gzip.open('redditSubmissions.csv.gz', 'rb') as data_in:\n",
        "    with open('submissions.csv', 'wb') as data_out:\n",
        "      data_out.write(data_in.read())\n",
        "  print('File downloaded')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU9QmAAFywGe",
        "outputId": "12426568-16ca-413c-bc7f-9873df4c80d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reading the dataset\n",
        "#lines that don't match the data structure are skipped\n",
        "dataset = df.read_csv('submissions.csv', on_bad_lines='warn',\n",
        "                      dtype={'localtime': 'float64',\n",
        "                             'number_of_comments': 'float64',\n",
        "                             'number_of_downvotes': 'float64',\n",
        "                             'number_of_upvotes': 'float64',\n",
        "                             'score': 'float64',\n",
        "                             'total_votes': 'float64',\n",
        "                             'unixtime': 'float64'})\n",
        "print(dataset)\n",
        "print(dataset.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpGtuoq-CQ91",
        "outputId": "e48c9263-6574-4b39-e092-ab48fa16866f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dask DataFrame Structure:\n",
            "              #image_id unixtime rawtime   title total_votes reddit_id number_of_upvotes subreddit number_of_downvotes localtime    score number_of_comments username\n",
            "npartitions=1                                                                                                                                                        \n",
            "                  int64  float64  string  string     float64    string           float64    string             float64   float64  float64            float64   string\n",
            "                    ...      ...     ...     ...         ...       ...               ...       ...                 ...       ...      ...                ...      ...\n",
            "Dask Name: to_string_dtype, 2 expressions\n",
            "Expr=ArrowStringConversion(frame=FromMapProjectable(34ca8c0))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/io/csv.py:199: ParserWarning: Skipping line 67828: expected 13 fields, saw 16\n",
            "Skipping line 67829: expected 13 fields, saw 16\n",
            "Skipping line 67830: expected 13 fields, saw 16\n",
            "Skipping line 67831: expected 13 fields, saw 16\n",
            "Skipping line 67832: expected 13 fields, saw 14\n",
            "\n",
            "  df = reader(bio, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   #image_id      unixtime                           rawtime  \\\n",
            "0          0  1.333172e+09  2012-03-31T12:40:39.590113-07:00   \n",
            "1          0  1.333178e+09  2012-03-31T14:16:01.093638-07:00   \n",
            "2          0  1.333200e+09  2012-03-31T20:18:33.192906-07:00   \n",
            "3          0  1.333252e+09         2012-04-01T10:52:10-07:00   \n",
            "4          0  1.333273e+09  2012-04-01T16:35:54.393381-07:00   \n",
            "\n",
            "                             title  total_votes reddit_id  number_of_upvotes  \\\n",
            "0           And here's a downvote.      63470.0     rmqjs            32657.0   \n",
            "1                      Expectation         35.0     rmun4               29.0   \n",
            "2                         Downvote         41.0     rna86               32.0   \n",
            "3  Every time I downvote something         10.0     ro7e4                6.0   \n",
            "4   Downvote &quot;Dies Irae&quot;         65.0     rooof               57.0   \n",
            "\n",
            "  subreddit  number_of_downvotes     localtime   score  number_of_comments  \\\n",
            "0     funny              30813.0  1.333198e+09  1844.0               622.0   \n",
            "1  GifSound                  6.0  1.333203e+09    23.0                 3.0   \n",
            "2  GifSound                  9.0  1.333225e+09    23.0                 0.0   \n",
            "3  GifSound                  4.0  1.333278e+09     2.0                 0.0   \n",
            "4  GifSound                  8.0  1.333298e+09    49.0                 0.0   \n",
            "\n",
            "              username  \n",
            "0  Animates_Everything  \n",
            "1        Gangsta_Raper  \n",
            "2        Gangsta_Raper  \n",
            "3        Gangsta_Raper  \n",
            "4        Gangsta_Raper  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Filtering Data**\n",
        "Filtering the data so that only the necessary components are included in the dataset ('title', 'subreddit', 'score', 'username', 'total_votes','number_of_upvotes', 'number_of_downvotes', 'number_of_comments')\n",
        "\n",
        "Explain why?\n",
        "number of upvotes and downvotes rather than score of post to compare which one is higher.\n",
        "- score just to see overall\n"
      ],
      "metadata": {
        "id": "9Q9WqMkS1UxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data = dataset[['title', 'subreddit', 'username', 'total_votes','number_of_upvotes', 'number_of_downvotes', 'number_of_comments']]\n",
        "print(filtered_data)\n",
        "print(filtered_data.head())\n",
        "print(filtered_data['title'].compute())"
      ],
      "metadata": {
        "id": "eaamAR6r1T1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4e94d0-931b-4e90-8a66-51667a371b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dask DataFrame Structure:\n",
            "                title subreddit username total_votes number_of_upvotes number_of_downvotes number_of_comments\n",
            "npartitions=1                                                                                                \n",
            "               string    string   string     float64           float64             float64            float64\n",
            "                  ...       ...      ...         ...               ...                 ...                ...\n",
            "Dask Name: getitem, 3 expressions\n",
            "Expr=ArrowStringConversion(frame=FromMapProjectable(34ca8c0))[['title', 'subreddit', 'username', 'total_votes', 'number_of_upvotes', 'number_of_downvotes', 'number_of_comments']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/io/csv.py:199: ParserWarning: Skipping line 67828: expected 13 fields, saw 16\n",
            "Skipping line 67829: expected 13 fields, saw 16\n",
            "Skipping line 67830: expected 13 fields, saw 16\n",
            "Skipping line 67831: expected 13 fields, saw 16\n",
            "Skipping line 67832: expected 13 fields, saw 14\n",
            "\n",
            "  df = reader(bio, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             title subreddit             username  \\\n",
            "0           And here's a downvote.     funny  Animates_Everything   \n",
            "1                      Expectation  GifSound        Gangsta_Raper   \n",
            "2                         Downvote  GifSound        Gangsta_Raper   \n",
            "3  Every time I downvote something  GifSound        Gangsta_Raper   \n",
            "4   Downvote &quot;Dies Irae&quot;  GifSound        Gangsta_Raper   \n",
            "\n",
            "   total_votes  number_of_upvotes  number_of_downvotes  number_of_comments  \n",
            "0      63470.0            32657.0              30813.0               622.0  \n",
            "1         35.0               29.0                  6.0                 3.0  \n",
            "2         41.0               32.0                  9.0                 0.0  \n",
            "3         10.0                6.0                  4.0                 0.0  \n",
            "4         65.0               57.0                  8.0                 0.0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/io/csv.py:199: ParserWarning: Skipping line 67828: expected 13 fields, saw 16\n",
            "Skipping line 67829: expected 13 fields, saw 16\n",
            "Skipping line 67830: expected 13 fields, saw 16\n",
            "Skipping line 67831: expected 13 fields, saw 16\n",
            "Skipping line 67832: expected 13 fields, saw 14\n",
            "\n",
            "  df = reader(bio, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                    And here's a downvote.\n",
            "1                                               Expectation\n",
            "2                                                  Downvote\n",
            "3                           Every time I downvote something\n",
            "4                            Downvote &quot;Dies Irae&quot;\n",
            "                                ...                        \n",
            "132298                                           OM NOM NOM\n",
            "132299                            Don't feed the animals...\n",
            "132300                                          WTF worthy.\n",
            "132301    Just a camel eating a kids head, welcome to th...\n",
            "132302      Ass is looking at a little girl eaten by camel.\n",
            "Name: title, Length: 132303, dtype: string\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cleaning Data**\n",
        "\n",
        "We turn the titles into lower case to avoid the problem of duplicates with capitalization.\n",
        "\n",
        "Afterwards, we remove stopwords to remove common words that don't add much meaning to our analysis as well as punctuation.\n"
      ],
      "metadata": {
        "id": "NShGN1xd_rlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "#turning titles into lowercase words to make it easier when analysing\n",
        "#filtered_data['title'] = filtered_data['title'].str.lower()\n",
        "#testing\n",
        "#filtered_data['title'].compute()\n",
        "\n",
        "#removing stopwords\n",
        "#delete later: (list of words, subreddit, engagement score)\n",
        "def clean_titles(title):\n",
        "  valid_words = []\n",
        "  if isinstance(title, str) and pd.notna(title):\n",
        "    title = re.sub(r'[^\\w\\s]', '', title)\n",
        "    for word in title.split(\" \"):\n",
        "      if len(word) > 0 and word not in stop_words1:\n",
        "        valid_words.append(word.lower())\n",
        "  else:\n",
        "    return[]\n",
        "  return \" \".join(valid_words)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a4H59JAHBzYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to filter the titles\n",
        "\n",
        "Engagement ratios are calculated to use as metrics"
      ],
      "metadata": {
        "id": "EqCqTrjxVZIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data['title'] = filtered_data['title'].map(clean_titles, meta=filtered_data['title'])\n",
        "#testing whether the titles are as expected\n",
        "filtered_data_result = filtered_data['title'].compute()\n",
        "print(filtered_data_result)\n",
        "#printing the first 5 results\n",
        "\n",
        "#Creating an engagement ratio to show how users interact with each post\n",
        "filtered_data['engagement ratio'] = filtered_data['number_of_upvotes']/filtered_data['total_votes']\n",
        "print(filtered_data.head())"
      ],
      "metadata": {
        "id": "elyfmLgwEqIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6585acc3-9871-4f38-9e41-a91770b85575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/io/csv.py:199: ParserWarning: Skipping line 67828: expected 13 fields, saw 16\n",
            "Skipping line 67829: expected 13 fields, saw 16\n",
            "Skipping line 67830: expected 13 fields, saw 16\n",
            "Skipping line 67831: expected 13 fields, saw 16\n",
            "Skipping line 67832: expected 13 fields, saw 14\n",
            "\n",
            "  df = reader(bio, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                   and heres downvote\n",
            "1                                          expectation\n",
            "2                                             downvote\n",
            "3                      every time i downvote something\n",
            "4                           downvote quotdies iraequot\n",
            "                              ...                     \n",
            "132298                                      om nom nom\n",
            "132299                               dont feed animals\n",
            "132300                                      wtf worthy\n",
            "132301    just camel eating kids head welcome internet\n",
            "132302             ass looking little girl eaten camel\n",
            "Name: title, Length: 132303, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/io/csv.py:199: ParserWarning: Skipping line 67828: expected 13 fields, saw 16\n",
            "Skipping line 67829: expected 13 fields, saw 16\n",
            "Skipping line 67830: expected 13 fields, saw 16\n",
            "Skipping line 67831: expected 13 fields, saw 16\n",
            "Skipping line 67832: expected 13 fields, saw 14\n",
            "\n",
            "  df = reader(bio, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             title subreddit             username  \\\n",
            "0               and heres downvote     funny  Animates_Everything   \n",
            "1                      expectation  GifSound        Gangsta_Raper   \n",
            "2                         downvote  GifSound        Gangsta_Raper   \n",
            "3  every time i downvote something  GifSound        Gangsta_Raper   \n",
            "4       downvote quotdies iraequot  GifSound        Gangsta_Raper   \n",
            "\n",
            "   total_votes  number_of_upvotes  number_of_downvotes  number_of_comments  \\\n",
            "0      63470.0            32657.0              30813.0               622.0   \n",
            "1         35.0               29.0                  6.0                 3.0   \n",
            "2         41.0               32.0                  9.0                 0.0   \n",
            "3         10.0                6.0                  4.0                 0.0   \n",
            "4         65.0               57.0                  8.0                 0.0   \n",
            "\n",
            "   engagement ratio  \n",
            "0          0.514527  \n",
            "1          0.828571  \n",
            "2          0.780488  \n",
            "3          0.600000  \n",
            "4          0.876923  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Grouping words by their subreddits**\n",
        "\n",
        "Grouping the words from post titles by their subreddits allow us to find words of importance in each subreddit (TF-IDF).\n",
        "With this we may find common themes, and may be able to compare similarities between subreddits.\n",
        "\n",
        "**To help perform TF-IDF, we will have to join all of the titles together to form a sort of document.**"
      ],
      "metadata": {
        "id": "i0GrYY29TcQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a string combining all of the post titles SPECIFICALLY for TF-IDF\n",
        "#group_data = filtered_data.groupby('subreddit')['title'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "#print(filtered_data['subreddit'].unique().compute())\n",
        "\n",
        "group_data = filtered_data[['title', 'subreddit']].compute()\n",
        "print(group_data.head())\n",
        "\n",
        "subreddit_data = group_data.groupby('subreddit')['title'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "print(subreddit_data.head())\n",
        "print(f\"There are {subreddit_data['subreddit'].unique()} amount of rows\")\n",
        "\n",
        "for x in range(5):\n",
        "  print(subreddit_data['title'][x])\n",
        "  #print(subreddit_data['subreddit'][x])\n",
        "#print(subreddit_data.compute())"
      ],
      "metadata": {
        "id": "X00u1HEiTO6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b09403c-f274-4763-fd5f-9381efed5839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/io/csv.py:199: ParserWarning: Skipping line 67828: expected 13 fields, saw 16\n",
            "Skipping line 67829: expected 13 fields, saw 16\n",
            "Skipping line 67830: expected 13 fields, saw 16\n",
            "Skipping line 67831: expected 13 fields, saw 16\n",
            "Skipping line 67832: expected 13 fields, saw 14\n",
            "\n",
            "  df = reader(bio, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             title subreddit\n",
            "0               and heres downvote     funny\n",
            "1                      expectation  GifSound\n",
            "2                         downvote  GifSound\n",
            "3  every time i downvote something  GifSound\n",
            "4       downvote quotdies iraequot  GifSound\n",
            "   subreddit                                              title\n",
            "0  2006Scape  as soon i finished manually installing cache h...\n",
            "1     2XLite  my boyfriend brought home crapload chocolate w...\n",
            "2     30ROCK                                im actually jealous\n",
            "3        311                                    pumpkin carving\n",
            "4      4chan  4chan really slow awesome slowpoke and thus go...\n",
            "There are <ArrowStringArray>\n",
            "[           '2006Scape',               '2XLite',               '30ROCK',\n",
            "                  '311',                '4chan',             '911truth',\n",
            "                 '9gag',              ': funny',                  'AMA',\n",
            " 'A_Monocle_For_Sauron',\n",
            " ...\n",
            "        'worldpolitics',                  'wow',          'wrongnumber',\n",
            "               'wtfart',              'xbox360',                 'xkcd',\n",
            "      'youtubecomments',                'zelda',             'zoidberg',\n",
            "              'zombies']\n",
            "Length: 867, dtype: string amount of rows\n",
            "as soon i finished manually installing cache heard login screen music\n",
            "my boyfriend brought home crapload chocolate week period\n",
            "im actually jealous\n",
            "pumpkin carving\n",
            "4chan really slow awesome slowpoke and thus goose loose 420 blaze faggot gtgf prego b makes captcha comics jabba 1 christiansatheistssolo 0 this saddest story ive seen 4chan quotno one ever believe youquot 4chan discovers humanoid creature google street view finally hitler si unit passing b best idea ever the nigger project recipe butthurt post quotsup b dig new threadsquot man gets deniggered revolutionary breakthrough science by founding fathers is that a polo shirt gtbe 0 op goes america i saw someone b posted better funnier version does anyone else if i the tale fats mcgee retard three too many feels my favorite image ever dont drugs pikachu deadlifting itt pretend redditors this depressing thing ive ever read found rage thread b monsters john kenn edited co spaceghostspitjpg spaceghostspitjpg oh b dolphins vs women these guys run shit someone call cops cause baby get plowed hey girl im bit artist would like sketch a little bit artist jesus smells nice new from mountain dew anon loses virginity anon takes pigeon gamestop when god created op pol_has_joined_the_servergif 4chans admins arent keen reddit korl lori is die no b rage comic sp goes movies the gym oh potato gtthat feel r9k back cop made op ruin computer anon turns alpha the fate b anon goes school trip op goes movies anons story thin privilege everything touch turns shit the epic threads reddit according b punch really hard checkmate feels someone well deserved bad time will update find the protector bargain hunter texting driving fuck suicide how feels found rage thread fuck dem asians how hell work handy cam helicopter penis op fun child you nerds dont know meaning embarrassment bad taxidermy thread anon hugs girl gtbe bear wife material itt under 16 sex stories bs secret sexual fantasy well awkward 4chan 9gags relationship this text i sent sister died car crash how meet girlguy dreams 810 would listen awww yeeeeeeeeeeah not everyone b bad b guesses girls names another picture op how redditor perceives idea chan dog blowjob turkey 4chan shows support 9gag cat problem anon telling truth running girl op tries lsd v gives buying advice fight 9gag xpost rfunny repost awareness b organizing happen 911 feeling bit bloated forgive america anon easily desktop thread ainsley makes one fb 4 panels b connects dots photoshop potential masters photoshop dont know thread b battletoads ops plan anons masturbation story 4chan operation kinder it already reached 25th place time post created why i feel guy came 4chan bs election predictions the difference look taylor swift contest 4chan versus internet op born punishment the anne frank time travel conspiracy fit gives advice fight dyel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Grouping subreddits from metrics**\n",
        "\n",
        "Creating a new dataframe that calculates metrics of the dataset. This will be merged with the dataset grouped by metrics. This way we have a string of titles that can be used as a document for TF-IDF, and engagement metrics that can be used to determine subreddits with popular posts and how they're interacted.\n",
        "\n"
      ],
      "metadata": {
        "id": "tYbU5PR7CQ2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "metric_data = filtered_data[['subreddit', 'total_votes', 'engagement ratio', 'number_of_comments']]\n",
        "#print(metric_data.head())\n",
        "#caclulating metrics\n",
        "engagement_metadata = metric_data.groupby('subreddit').agg({'total_votes': np.mean, 'engagement ratio': np.mean, 'number_of_comments': np.mean}).compute().reset_index()\n",
        "engagement_metadata.columns = ['subreddit', 'avg_total_votes', 'avg_engagement_ratio', 'avg_comments']\n",
        "\n",
        "\n",
        "#adding a post count for each subreddit to ensure subreddits with less than x aren't included.\n",
        "post_counts = filtered_data.groupby('subreddit').size().compute().reset_index(name='post_count')\n",
        "\n",
        "#merging the post counts to match their subreddits\n",
        "engagement_metadata = engagement_metadata.merge(post_counts, on='subreddit', how='left')\n",
        "\n",
        "#keeping only subreddits where the post count > 100 to eliminate the possibility of low post count\n",
        "#in a subreddit reddit but high number of votes that can skew the data.\n",
        "engagement_metadata = engagement_metadata.query('post_count > 1000')\n",
        "\n",
        "print(engagement_metadata.head())"
      ],
      "metadata": {
        "id": "mCffa2__Kf3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e313fc-476d-49df-e99f-54e280d4210e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/io/csv.py:199: ParserWarning: Skipping line 67828: expected 13 fields, saw 16\n",
            "Skipping line 67829: expected 13 fields, saw 16\n",
            "Skipping line 67830: expected 13 fields, saw 16\n",
            "Skipping line 67831: expected 13 fields, saw 16\n",
            "Skipping line 67832: expected 13 fields, saw 14\n",
            "\n",
            "  df = reader(bio, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/io/csv.py:199: ParserWarning: Skipping line 67828: expected 13 fields, saw 16\n",
            "Skipping line 67829: expected 13 fields, saw 16\n",
            "Skipping line 67830: expected 13 fields, saw 16\n",
            "Skipping line 67831: expected 13 fields, saw 16\n",
            "Skipping line 67832: expected 13 fields, saw 14\n",
            "\n",
            "  df = reader(bio, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  subreddit  avg_total_votes  avg_engagement_ratio  avg_comments  post_count\n",
            "0     funny      2460.074172              0.596047     36.929501       55277\n",
            "1  GifSound        27.007311              0.658663      0.873217        5608\n",
            "2      gifs       884.646116              0.647087     17.233929       12538\n",
            "4      pics      1866.567255              0.630789     48.873624       24712\n",
            "5   atheism      1530.366310              0.649735     63.202614        3366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Merging the two datasets**\n",
        "\n",
        "We will merge the two datasets to create a singular dataset that can be used for TF-IDF and engagement interpretation.\n",
        "\n",
        "We will also be finding out the most engaged with subreddit. With this we may also find out how the subreddit is being interacted with."
      ],
      "metadata": {
        "id": "6f4LfZ-0xCWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df.merge(engagement_metadata, subreddit_data, on='subreddit', how='inner')\n",
        "#print(dataset.head())\n",
        "\n",
        "ranked_dataset = dataset.sort_values(by='avg_total_votes', ascending=False).head(10)\n",
        "print(ranked_dataset)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7nJ_PYXxBx8",
        "outputId": "d64969e5-68f7-425e-da3f-ce677bc775fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  subreddit  avg_total_votes  avg_engagement_ratio  avg_comments  post_count  \\\n",
            "0     funny      2460.074172              0.596047     36.929501       55277   \n",
            "5    gaming      2065.452310              0.568028     64.352609        3009   \n",
            "3      pics      1866.567255              0.630789     48.873624       24712   \n",
            "6       WTF      1552.849012              0.579460     57.880587       12193   \n",
            "4   atheism      1530.366310              0.649735     63.202614        3366   \n",
            "7       aww      1488.294764              0.741335     19.505236        5920   \n",
            "2      gifs       884.646116              0.647087     17.233929       12538   \n",
            "1  GifSound        27.007311              0.658663      0.873217        5608   \n",
            "\n",
            "                                               title  \n",
            "0  and heres downvote getting first downvote new ...  \n",
            "5  sitting new knight rgaming seeing insanely twi...  \n",
            "3  every lastairbender post nsfw tag ohhh thats l...  \n",
            "6  the faces definitely one weirder gifs ive come...  \n",
            "4  the reaction face express view religion cheesu...  \n",
            "7  if i get otter im jumping in but cold if i get...  \n",
            "2  demolished every time downvote someone how i f...  \n",
            "1  expectation downvote every time i downvote som...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TF-IDF implementation**"
      ],
      "metadata": {
        "id": "6H_1jHFRTbyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#Creating a TF-IDF dataframe for manipulation\n",
        "tfidf = TfidfVectorizer(stop_words = 'english')\n",
        "\n",
        "tfidf_matrix = tfidf.fit_transform(ranked_dataset['title'])\n",
        "\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    index=ranked_dataset['subreddit'],\n",
        "    columns= tfidf.get_feature_names_out()\n",
        ")\n",
        "\n",
        "\n",
        "#Looking for the top 20 words\n",
        "def top_20_words(row, n=10):\n",
        "  top_words = row.argsort()[-n:][::-1]\n",
        "  words_list = []\n",
        "  for i in top_words:\n",
        "    word = tfidf_df.columns[i]\n",
        "    words_list.append((word, row.iloc[i]))\n",
        "  return words_list\n",
        "\n",
        "#looking for the top 20 words in each subreddit\n",
        "tfidf_wordlist = tfidf_df.apply(lambda row: top_20_words(row), axis=1)\n",
        "print(tfidf_wordlist)\n",
        "\n"
      ],
      "metadata": {
        "id": "s1kh0nTyOgqh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1319cee9-567f-409d-cc80-a0d7943c5c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subreddit\n",
            "funny       [(feel, 0.41473858503511), (reddit, 0.26381010...\n",
            "gaming      [(game, 0.48967949353646056), (steam, 0.238072...\n",
            "pics        [(like, 0.2424917049179486), (picture, 0.22176...\n",
            "WTF         [(wtf, 0.3680641077292314), (nsfw, 0.350736414...\n",
            "atheism     [(jesus, 0.32237073296561986), (religion, 0.26...\n",
            "aww         [(baby, 0.4528238432809415), (cute, 0.30622081...\n",
            "gifs        [(feel, 0.3945808729910792), (gif, 0.336538612...\n",
            "GifSound    [(gif, 0.2577075557981244), (cat, 0.2457608479...\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cosine Similarity**\n",
        "Calculating cosine similarity will allow us to see which subreddits have high similarity. From here we may use this to compare how each subreddit is interacted with differently."
      ],
      "metadata": {
        "id": "cuC4WH8zNgjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(u, v):\n",
        "    dot_product = np.dot(u,v)\n",
        "    norm_u = (sum([(u[i])**2 for i in range(len(u))]))**(1/2)\n",
        "    norm_v = (sum([(v[i])**2 for i in range(len(u))]))**(1/2)\n",
        "\n",
        "    return dot_product/(norm_u * norm_v)\n",
        "\n",
        "\n",
        "tfidf_wordlist_values = tfidf_wordlist.map(lambda row: [x[1] for x in row])\n",
        "\n",
        "sub_names = tfidf_df.index.tolist()\n",
        "vectors = tfidf_df.values\n",
        "print(sub_names)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xv0htRP4Nf5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb3dc866-e760-4bbc-9108-2c642d9c19d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['funny', 'gaming', 'pics', 'WTF', 'atheism', 'aww', 'gifs', 'GifSound']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparing engagement ratios along with similarities**\n",
        "\n",
        "The engagement ratio for each word is calculated by taking the average engagement ratio from all of the posts in the unique subreddit where the word is present.\n",
        "This will allow use to see how differently posts with these specific words are interacted with."
      ],
      "metadata": {
        "id": "tkC71zoLMoRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #ranked_dataset.head(10)\n",
        "\n",
        "#calculating the type of engagement each word has in their respective subreddits\n",
        "tfidf_wordlist_values = tfidf_wordlist.map(lambda row: [x[0] for x in row])\n",
        "print(tfidf_wordlist_values)\n",
        "\n",
        "#using the filtered data to use the engagement ratio of each post\n",
        "filtered_data['title_list'] = filtered_data['title'].str.split()\n",
        "#print(filtered_data.head())\n",
        "\n",
        "###Adding average engagement ratios for each individual word to show the type of engagement each word is associated with in posts in their respective subreddit\n",
        "#Titles need to be exploded to help compute the engagement ratios\n",
        "exploded_titles = filtered_data.explode('title_list')\n",
        "word_engagement = (exploded_titles.groupby(['subreddit', 'title_list'])['engagement ratio'].agg(['mean', 'count']).reset_index()\n",
        "    .rename(columns={'title_list': 'word'})\n",
        "    .rename(columns={'mean': 'avg_engagement_ratio','count': 'post_count'})).compute()\n",
        "#print(word_engagement)\n",
        "\n",
        "\n",
        "#This list will be used to create a dataframe\n",
        "rows = []\n",
        "for subreddit, word_list in tfidf_wordlist.items():\n",
        "    for word, score in word_list:\n",
        "        rows.append({'subreddit': subreddit, 'word': word, 'tfidf_score': score})\n",
        "\n",
        "#creates a dataframe that can be merged with word engagement (only the words that are in the top 20)w\n",
        "top_words_flat = pd.DataFrame(rows)\n",
        "merged = top_words_flat.merge(word_engagement, on=['subreddit', 'word'], how='left')\n",
        "print(merged)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xZuV8q_KGWF",
        "outputId": "7fdfec74-9fdb-4503-e5ae-3313fb2dee24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subreddit\n",
            "funny       [feel, reddit, time, like, day, im, post, dont...\n",
            "gaming      [game, steam, playing, time, gaming, games, fi...\n",
            "pics        [like, picture, reddit, just, time, day, new, ...\n",
            "WTF         [wtf, nsfw, dont, like, fuck, just, know, thin...\n",
            "atheism     [jesus, religion, atheist, god, atheists, chri...\n",
            "aww         [baby, cute, cat, little, just, kitty, dog, cu...\n",
            "gifs        [feel, gif, time, reddit, day, like, post, im,...\n",
            "GifSound    [gif, cat, like, fixed, im, song, dance, time,...\n",
            "dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/io/csv.py:199: ParserWarning: Skipping line 67828: expected 13 fields, saw 16\n",
            "Skipping line 67829: expected 13 fields, saw 16\n",
            "Skipping line 67830: expected 13 fields, saw 16\n",
            "Skipping line 67831: expected 13 fields, saw 16\n",
            "Skipping line 67832: expected 13 fields, saw 14\n",
            "\n",
            "  df = reader(bio, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   subreddit    word  tfidf_score  avg_engagement_ratio  post_count\n",
            "0      funny    feel     0.414739              0.610574        2965\n",
            "1      funny  reddit     0.263810              0.601518        1886\n",
            "2      funny    time     0.236114              0.605077        1688\n",
            "3      funny    like     0.222546              0.600848        1591\n",
            "4      funny     day     0.176946              0.604524        1265\n",
            "..       ...     ...          ...                   ...         ...\n",
            "75  GifSound    song     0.179323              0.640083          94\n",
            "76  GifSound   dance     0.165547              0.688728          97\n",
            "77  GifSound    time     0.151894              0.678329          89\n",
            "78  GifSound    dont     0.134827              0.663854          79\n",
            "79  GifSound   party     0.133120              0.613662          78\n",
            "\n",
            "[80 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Printing the results of TF-IDF and Cosine Similarity**"
      ],
      "metadata": {
        "id": "DHH7yfNqIUjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_by_subreddit = merged.groupby('subreddit')\n",
        "\n",
        "#The type of engagement of each word\n",
        "#for each subreddit, we print their rows.\n",
        "\n",
        "print('TF-IDF Table: \\n')\n",
        "print('This calculates the number of a word to the subreddit.\\n Average Word Engagement ratio takes the average engagement of all the posts the word is present in.\\n')\n",
        "print('Average Engagement Ratios < 0.5 mean posts containing the word have more negative interactions. Scores > 0.5 mean more positive interactions')\n",
        "for subreddit_name, subreddit_data in grouped_by_subreddit:\n",
        "    print(f\"Subreddit: {subreddit_name}\")\n",
        "\n",
        "    subreddit_data_sorted = subreddit_data.sort_values(by='tfidf_score', ascending=False)\n",
        "\n",
        "    for index, row in subreddit_data_sorted.iterrows():\n",
        "        word = row['word']\n",
        "        tfidf_score = row['tfidf_score']\n",
        "        avg_engagement_ratio = row['avg_engagement_ratio']\n",
        "        post_count = row['post_count'] # Include post count for more context\n",
        "        print(f\"  Word: {word} {' ' * (10- len(word))} TF-IDF Score: {tfidf_score:.4f}  {' ' * 5}Avg Word Engagement Ratio: {avg_engagement_ratio:.4f}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "print('\\nThe Cosine Similarity matrix shows the similarities between two subreddits.\\nRatio scores closer to 1 indicate high similarity of emphasized words and topics')\n",
        "#matrix for the cosine similarity\n",
        "cos_sim_matrix = pd.DataFrame()\n",
        "#values to be added into the matrix\n",
        "for x in range(len(sub_names)):\n",
        "    for y in range(len(sub_names)):\n",
        "        similarity = cosine_similarity(vectors[x], vectors[y])\n",
        "        cos_sim_matrix.loc[sub_names[x], sub_names[y]] = similarity\n",
        "print(cos_sim_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d3ZhMQCH0zX",
        "outputId": "7acdcf0f-ed6c-400d-f49f-8de2e6c9aa46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Table: \n",
            "\n",
            "This calculates the number of a word to the subreddit.\n",
            " Average Word Engagement ratio takes the average engagement of all the posts the word is present in.\n",
            "\n",
            "Average Engagement Ratios < 0.5 mean posts containing the word have more negative interactions. Scores > 0.5 mean more positive interactions\n",
            "Subreddit: GifSound\n",
            "  Word: gif         TF-IDF Score: 0.2577       Avg Word Engagement Ratio: 0.6379\n",
            "  Word: cat         TF-IDF Score: 0.2458       Avg Word Engagement Ratio: 0.6738\n",
            "  Word: like        TF-IDF Score: 0.2202       Avg Word Engagement Ratio: 0.6862\n",
            "  Word: fixed       TF-IDF Score: 0.2065       Avg Word Engagement Ratio: 0.6514\n",
            "  Word: im          TF-IDF Score: 0.1946       Avg Word Engagement Ratio: 0.6514\n",
            "  Word: song        TF-IDF Score: 0.1793       Avg Word Engagement Ratio: 0.6401\n",
            "  Word: dance       TF-IDF Score: 0.1655       Avg Word Engagement Ratio: 0.6887\n",
            "  Word: time        TF-IDF Score: 0.1519       Avg Word Engagement Ratio: 0.6783\n",
            "  Word: dont        TF-IDF Score: 0.1348       Avg Word Engagement Ratio: 0.6639\n",
            "  Word: party       TF-IDF Score: 0.1331       Avg Word Engagement Ratio: 0.6137\n",
            "\n",
            "\n",
            "Subreddit: WTF\n",
            "  Word: wtf         TF-IDF Score: 0.3681       Avg Word Engagement Ratio: 0.5666\n",
            "  Word: nsfw        TF-IDF Score: 0.3507       Avg Word Engagement Ratio: 0.5664\n",
            "  Word: dont        TF-IDF Score: 0.2934       Avg Word Engagement Ratio: 0.5728\n",
            "  Word: like        TF-IDF Score: 0.1745       Avg Word Engagement Ratio: 0.5792\n",
            "  Word: fuck        TF-IDF Score: 0.1697       Avg Word Engagement Ratio: 0.5701\n",
            "  Word: just        TF-IDF Score: 0.1476       Avg Word Engagement Ratio: 0.5714\n",
            "  Word: know        TF-IDF Score: 0.1422       Avg Word Engagement Ratio: 0.5785\n",
            "  Word: think       TF-IDF Score: 0.1410       Avg Word Engagement Ratio: 0.5860\n",
            "  Word: nope        TF-IDF Score: 0.1383       Avg Word Engagement Ratio: 0.5826\n",
            "  Word: xpost       TF-IDF Score: 0.1374       Avg Word Engagement Ratio: 0.6101\n",
            "\n",
            "\n",
            "Subreddit: atheism\n",
            "  Word: jesus       TF-IDF Score: 0.3224       Avg Word Engagement Ratio: 0.6154\n",
            "  Word: religion    TF-IDF Score: 0.2683       Avg Word Engagement Ratio: 0.6492\n",
            "  Word: atheist     TF-IDF Score: 0.2664       Avg Word Engagement Ratio: 0.6447\n",
            "  Word: god         TF-IDF Score: 0.2383       Avg Word Engagement Ratio: 0.6441\n",
            "  Word: atheists    TF-IDF Score: 0.2300       Avg Word Engagement Ratio: 0.6391\n",
            "  Word: christians  TF-IDF Score: 0.2109       Avg Word Engagement Ratio: 0.6656\n",
            "  Word: bible       TF-IDF Score: 0.1741       Avg Word Engagement Ratio: 0.6663\n",
            "  Word: xpost       TF-IDF Score: 0.1625       Avg Word Engagement Ratio: 0.6663\n",
            "  Word: like        TF-IDF Score: 0.1584       Avg Word Engagement Ratio: 0.6284\n",
            "  Word: ratheism    TF-IDF Score: 0.1522       Avg Word Engagement Ratio: 0.6492\n",
            "\n",
            "\n",
            "Subreddit: aww\n",
            "  Word: baby        TF-IDF Score: 0.4528       Avg Word Engagement Ratio: 0.7506\n",
            "  Word: cute        TF-IDF Score: 0.3062       Avg Word Engagement Ratio: 0.7353\n",
            "  Word: cat         TF-IDF Score: 0.2758       Avg Word Engagement Ratio: 0.7026\n",
            "  Word: little      TF-IDF Score: 0.2184       Avg Word Engagement Ratio: 0.7344\n",
            "  Word: just        TF-IDF Score: 0.2033       Avg Word Engagement Ratio: 0.7528\n",
            "  Word: kitty       TF-IDF Score: 0.1814       Avg Word Engagement Ratio: 0.7242\n",
            "  Word: dog         TF-IDF Score: 0.1666       Avg Word Engagement Ratio: 0.6934\n",
            "  Word: cutest      TF-IDF Score: 0.1575       Avg Word Engagement Ratio: 0.7481\n",
            "  Word: kitten      TF-IDF Score: 0.1357       Avg Word Engagement Ratio: 0.7306\n",
            "  Word: puppy       TF-IDF Score: 0.1308       Avg Word Engagement Ratio: 0.7330\n",
            "\n",
            "\n",
            "Subreddit: funny\n",
            "  Word: feel        TF-IDF Score: 0.4147       Avg Word Engagement Ratio: 0.6106\n",
            "  Word: reddit      TF-IDF Score: 0.2638       Avg Word Engagement Ratio: 0.6015\n",
            "  Word: time        TF-IDF Score: 0.2361       Avg Word Engagement Ratio: 0.6051\n",
            "  Word: like        TF-IDF Score: 0.2225       Avg Word Engagement Ratio: 0.6008\n",
            "  Word: day         TF-IDF Score: 0.1769       Avg Word Engagement Ratio: 0.6045\n",
            "  Word: im          TF-IDF Score: 0.1711       Avg Word Engagement Ratio: 0.6027\n",
            "  Word: post        TF-IDF Score: 0.1430       Avg Word Engagement Ratio: 0.5896\n",
            "  Word: dont        TF-IDF Score: 0.1420       Avg Word Engagement Ratio: 0.5968\n",
            "  Word: think       TF-IDF Score: 0.1211       Avg Word Engagement Ratio: 0.5889\n",
            "  Word: fixed       TF-IDF Score: 0.1190       Avg Word Engagement Ratio: 0.5428\n",
            "\n",
            "\n",
            "Subreddit: gaming\n",
            "  Word: game        TF-IDF Score: 0.4897       Avg Word Engagement Ratio: 0.6056\n",
            "  Word: steam       TF-IDF Score: 0.2381       Avg Word Engagement Ratio: 0.5584\n",
            "  Word: playing     TF-IDF Score: 0.1970       Avg Word Engagement Ratio: 0.5713\n",
            "  Word: time        TF-IDF Score: 0.1949       Avg Word Engagement Ratio: 0.5762\n",
            "  Word: gaming      TF-IDF Score: 0.1949       Avg Word Engagement Ratio: 0.5713\n",
            "  Word: games       TF-IDF Score: 0.1678       Avg Word Engagement Ratio: 0.5738\n",
            "  Word: fixed       TF-IDF Score: 0.1326       Avg Word Engagement Ratio: 0.4833\n",
            "  Word: sale        TF-IDF Score: 0.1293       Avg Word Engagement Ratio: 0.5674\n",
            "  Word: play        TF-IDF Score: 0.1248       Avg Word Engagement Ratio: 0.5654\n",
            "  Word: like        TF-IDF Score: 0.1232       Avg Word Engagement Ratio: 0.5888\n",
            "\n",
            "\n",
            "Subreddit: gifs\n",
            "  Word: feel        TF-IDF Score: 0.3946       Avg Word Engagement Ratio: 0.6375\n",
            "  Word: gif         TF-IDF Score: 0.3365       Avg Word Engagement Ratio: 0.6267\n",
            "  Word: time        TF-IDF Score: 0.2290       Avg Word Engagement Ratio: 0.6453\n",
            "  Word: reddit      TF-IDF Score: 0.2284       Avg Word Engagement Ratio: 0.6359\n",
            "  Word: day         TF-IDF Score: 0.2162       Avg Word Engagement Ratio: 0.6216\n",
            "  Word: like        TF-IDF Score: 0.2018       Avg Word Engagement Ratio: 0.6492\n",
            "  Word: post        TF-IDF Score: 0.1603       Avg Word Engagement Ratio: 0.6029\n",
            "  Word: im          TF-IDF Score: 0.1432       Avg Word Engagement Ratio: 0.6554\n",
            "  Word: favorite    TF-IDF Score: 0.1289       Avg Word Engagement Ratio: 0.6457\n",
            "  Word: cake        TF-IDF Score: 0.1267       Avg Word Engagement Ratio: 0.5858\n",
            "\n",
            "\n",
            "Subreddit: pics\n",
            "  Word: like        TF-IDF Score: 0.2425       Avg Word Engagement Ratio: 0.6250\n",
            "  Word: picture     TF-IDF Score: 0.2218       Avg Word Engagement Ratio: 0.6162\n",
            "  Word: reddit      TF-IDF Score: 0.1847       Avg Word Engagement Ratio: 0.5917\n",
            "  Word: just        TF-IDF Score: 0.1752       Avg Word Engagement Ratio: 0.6311\n",
            "  Word: time        TF-IDF Score: 0.1458       Avg Word Engagement Ratio: 0.6094\n",
            "  Word: day         TF-IDF Score: 0.1400       Avg Word Engagement Ratio: 0.6335\n",
            "  Word: new         TF-IDF Score: 0.1323       Avg Word Engagement Ratio: 0.6015\n",
            "  Word: cat         TF-IDF Score: 0.1323       Avg Word Engagement Ratio: 0.6133\n",
            "  Word: dont        TF-IDF Score: 0.1316       Avg Word Engagement Ratio: 0.6160\n",
            "  Word: feel        TF-IDF Score: 0.1291       Avg Word Engagement Ratio: 0.6180\n",
            "\n",
            "\n",
            "\n",
            "The Cosine Similarity matrix shows the similarities between two subreddits.\n",
            "Ratio scores closer to 1 indicate high similarity of emphasized words and topics\n",
            "             funny    gaming      pics       WTF   atheism       aww  \\\n",
            "funny     1.000000  0.485133  0.797402  0.618613  0.493422  0.456960   \n",
            "gaming    0.485133  1.000000  0.446297  0.356517  0.282567  0.246071   \n",
            "pics      0.797402  0.446297  1.000000  0.663087  0.452528  0.547964   \n",
            "WTF       0.618613  0.356517  0.663087  1.000000  0.406063  0.423836   \n",
            "atheism   0.493422  0.282567  0.452528  0.406063  1.000000  0.243241   \n",
            "aww       0.456960  0.246071  0.547964  0.423836  0.243241  1.000000   \n",
            "gifs      0.911625  0.434926  0.693748  0.549393  0.405117  0.438976   \n",
            "GifSound  0.587080  0.358829  0.573205  0.484531  0.311855  0.450581   \n",
            "\n",
            "              gifs  GifSound  \n",
            "funny     0.911625  0.587080  \n",
            "gaming    0.434926  0.358829  \n",
            "pics      0.693748  0.573205  \n",
            "WTF       0.549393  0.484531  \n",
            "atheism   0.405117  0.311855  \n",
            "aww       0.438976  0.450581  \n",
            "gifs      1.000000  0.595884  \n",
            "GifSound  0.595884  1.000000  \n"
          ]
        }
      ]
    }
  ]
}